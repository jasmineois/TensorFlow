{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "予測モデルを作成する関数\n",
    "@param images_placeholder - 画像のplaceholder\n",
    "@param keep_prob - dropout率のplace_holder\n",
    "@return y_conv - 各クラスの確率(のようなもの)\n",
    "\"\"\"\n",
    "def inference(images_placeholder, keep_prob):\n",
    "\n",
    "    # 重みを標準偏差0.1の正規分布で初期化\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    # バイアスを標準偏差0.1の正規分布で初期化\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    # 畳み込み層の作成\n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # プーリング層の作成\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # 入力を28x28x3に変形\n",
    "    x_image = tf.reshape(images_placeholder, [-1, 28, 28, 3])\n",
    "\n",
    "    # 畳み込み層1の作成\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    # プーリング層1の作成\n",
    "    with tf.name_scope('pool1') as scope:\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    # 畳み込み層2の作成\n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    # プーリング層2の作成\n",
    "    with tf.name_scope('pool2') as scope:\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # 全結合層1の作成\n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        W_fc1 = weight_variable([7*7*64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        # dropoutの設定\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # 全結合層2の作成\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        W_fc2 = weight_variable([1024, NUM_CLASSES])\n",
    "        b_fc2 = bias_variable([NUM_CLASSES])\n",
    "\n",
    "    # ソフトマックス関数による正規化\n",
    "    with tf.name_scope('softmax') as scope:\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    # 各ラベルの確率のようなものを返す\n",
    "    return y_conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "lossを計算する関数\n",
    "@param logits - ロジットのtensor, float - [batch_size, NUM_CLASSES]\n",
    "@param labels -  ラベルのtensor, int32 - [batch_size, NUM_CLASSES]\n",
    "@return cross_entropy: 交差エントロピーのtensor, float\n",
    "\"\"\"\n",
    "def loss(logits, labels):\n",
    "    # 交差エントロピーの計算\n",
    "    cross_entropy = -tf.reduce_sum(labels*tf.log(logits))\n",
    "    # TensorBoardで表示するよう指定\n",
    "    tf.scalar_summary(\"cross_entropy\", cross_entropy)\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "訓練のOpを定義する関数\n",
    "@param loss - 損失のtensor, loss()の結果\n",
    "@param learning_rate - 学習係数\n",
    "@return train_step - 訓練のOp\n",
    "\"\"\"\n",
    "def training(loss, learning_rate):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "正解率(accuracy)を計算する関数\n",
    "@param logits - inference()の結果\n",
    "@param labels - ラベルのtensor, int32 - [batch_size, NUM_CLASSES]\n",
    "@return accuracy - 正解率(float)\n",
    "\"\"\"\n",
    "def accuracy(logits, labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    tf.scalar_summary(\"accuracy\", accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image/cut/ayana0.jpg\n",
      "image/cut/ayana1.jpg\n",
      "image/cut/ayana2.jpg\n",
      "image/cut/hana0.jpg\n",
      "image/cut/hana1.jpg\n",
      "image/cut/hana2.jpg\n",
      "image/cut/ayana0.jpg\n",
      "image/cut/hana0.jpg\n",
      "WARNING:tensorflow:From <ipython-input-30-00643faa19a1>:82 in loss.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-30-00643faa19a1>:106 in accuracy.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-31-6f5497a61ba9>:69 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-31-6f5497a61ba9>:72 in <module>.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From <ipython-input-31-6f5497a61ba9>:73 in <module>.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "step 0, training accuracy 0.5\n",
      "step 1, training accuracy 0.666667\n",
      "step 2, training accuracy 0.5\n",
      "step 3, training accuracy 0.5\n",
      "step 4, training accuracy 0.5\n",
      "step 5, training accuracy 0.5\n",
      "step 6, training accuracy 0.5\n",
      "step 7, training accuracy 0.833333\n",
      "step 8, training accuracy 0.833333\n",
      "step 9, training accuracy 1\n",
      "step 10, training accuracy 1\n",
      "step 11, training accuracy 1\n",
      "step 12, training accuracy 1\n",
      "step 13, training accuracy 1\n",
      "step 14, training accuracy 1\n",
      "step 15, training accuracy 1\n",
      "step 16, training accuracy 1\n",
      "step 17, training accuracy 1\n",
      "step 18, training accuracy 1\n",
      "step 19, training accuracy 1\n",
      "step 20, training accuracy 1\n",
      "step 21, training accuracy 1\n",
      "step 22, training accuracy 1\n",
      "step 23, training accuracy 1\n",
      "step 24, training accuracy 1\n",
      "step 25, training accuracy 1\n",
      "step 26, training accuracy 1\n",
      "step 27, training accuracy 1\n",
      "step 28, training accuracy 1\n",
      "step 29, training accuracy 1\n",
      "step 30, training accuracy 1\n",
      "step 31, training accuracy 1\n",
      "step 32, training accuracy 1\n",
      "step 33, training accuracy 1\n",
      "step 34, training accuracy 1\n",
      "step 35, training accuracy 1\n",
      "step 36, training accuracy 1\n",
      "step 37, training accuracy 1\n",
      "step 38, training accuracy 1\n",
      "step 39, training accuracy 1\n",
      "step 40, training accuracy 1\n",
      "step 41, training accuracy 1\n",
      "step 42, training accuracy 1\n",
      "step 43, training accuracy 1\n",
      "step 44, training accuracy 1\n",
      "step 45, training accuracy 1\n",
      "step 46, training accuracy 1\n",
      "step 47, training accuracy 1\n",
      "step 48, training accuracy 1\n",
      "step 49, training accuracy 1\n",
      "test accuracy 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ファイルを開く\n",
    "    f = open('train.txt', 'r')\n",
    "    # データを入れる配列\n",
    "    train_image = []\n",
    "    train_label = []\n",
    "    for line in f:\n",
    "        # 改行を除いてスペース区切りにする\n",
    "        line = line.rstrip()\n",
    "        l = line.split()\n",
    "        # パス\n",
    "        p = 'image/cut/' + l[0]\n",
    "        print(p)\n",
    "        # データを読み込んで28x28に縮小\n",
    "        img = cv2.imread(p)\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        # 一列にした後、0-1のfloat値にする\n",
    "        train_image.append(img.flatten().astype(np.float32)/255.0)\n",
    "        # ラベルを1-of-k方式で用意する\n",
    "        tmp = np.zeros(NUM_CLASSES)\n",
    "        tmp[int(l[1])] = 1\n",
    "        train_label.append(tmp)\n",
    "    # numpy形式に変換\n",
    "    train_image = np.asarray(train_image)\n",
    "    train_label = np.asarray(train_label)\n",
    "    f.close()\n",
    "    # print(train_image)\n",
    "    # print(train_label)\n",
    "    \n",
    "    f = open('test.txt', 'r')\n",
    "    test_image = []\n",
    "    test_label = []\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        l = line.split()\n",
    "        p = 'image/cut/' + l[0]\n",
    "        print(p)\n",
    "        img = cv2.imread(p)\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        test_image.append(img.flatten().astype(np.float32)/255.0)\n",
    "        tmp = np.zeros(NUM_CLASSES)\n",
    "        tmp[int(l[1])] = 1\n",
    "        test_label.append(tmp)\n",
    "    test_image = np.asarray(test_image)\n",
    "    test_label = np.asarray(test_label)\n",
    "    f.close()\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        images_placeholder = tf.placeholder(\"float\", shape=(None, 28*28*3))\n",
    "        labels_placeholder = tf.placeholder(\"float\", shape=(None, 2))\n",
    "        keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "        # inference → loss → training\n",
    "        logits = inference(images_placeholder, keep_prob)\n",
    "        loss_value = loss(logits, labels_placeholder)\n",
    "        train_op = training(loss_value, 1e-4)\n",
    "        acc = accuracy(logits, labels_placeholder)\n",
    "\n",
    "        # Session\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # TensorBoardで表示する値の設定\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        summary_writer = tf.train.SummaryWriter('image/train/', sess.graph_def)\n",
    "        \n",
    "        # 訓練の実行\n",
    "        for step in range(50):\n",
    "            for i in range(1):\n",
    "                # batch_size分の画像に対して訓練の実行\n",
    "                batch = 10*i\n",
    "                # feed_dictでplaceholderに入れるデータを指定する\n",
    "                sess.run(train_op, feed_dict={\n",
    "                  images_placeholder: train_image[batch:batch+10],\n",
    "                  labels_placeholder: train_label[batch:batch+10],\n",
    "                  keep_prob: 0.5})\n",
    "\n",
    "            # 1 step終わるたびに精度を計算する\n",
    "            train_accuracy = sess.run(acc, feed_dict={\n",
    "                images_placeholder: train_image,\n",
    "                labels_placeholder: train_label,\n",
    "                keep_prob: 1.0})\n",
    "            print(\"step %d, training accuracy %g\"%(step, train_accuracy))\n",
    "\n",
    "            # 1 step終わるたびにTensorBoardに表示する値を追加する\n",
    "            summary_str = sess.run(summary_op, feed_dict={\n",
    "                images_placeholder: train_image,\n",
    "                labels_placeholder: train_label,\n",
    "                keep_prob: 1.0})\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        \n",
    "    # 訓練が終了したらテストデータに対する精度を表示\n",
    "    print(\"test accuracy %g\"%sess.run(acc, feed_dict={\n",
    "        images_placeholder: test_image,\n",
    "        labels_placeholder: test_label,\n",
    "        keep_prob: 1.0}))\n",
    "\n",
    "    # 最終的なモデルを保存\n",
    "    # save_path = saver.save(sess, \"model.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
